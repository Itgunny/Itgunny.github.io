###Logistic Classification의 가설 함수 정의

####Binary Classification : 둘중 하나를 판단하는 방법

1) Spam Detection : Spam(1) or Ham(0)

2) Facebook feed: show(1) or hide(0)

3) Credit Card Fraudulent Transaction detection: legitimate(1) / fraud(0)

예를들어 오래 공부한사람이 시험이 통과(1) / 실패(0)한다는 예를 가정으로 하자

Linear Regression을 적용하여도 오래 공부한 사람이 실패한 경우가 생긴다. 따라서 Linear Regression을 적용할 수 없는 경우 Logistic Classification을 적용해야 한다. 

또한 0과 1을 구하는 방법이므로 Linear Regression 적용시 0보다 작은 혹은 큰 값이 나올 수도 있다. 

### Logistic Hypothesis

$H(x)  = Wx + b​$ 을 변형하여 0 과 1로 수렴하는 그래프로 변형하였다. 

$G(z) = \frac{1}{1+e^{-z}}$ 함수를 적용하면 0과 1 사이에 값을 가져 올 수 있다. 이것을 sigmoid라고 부른다. 

따라서 이것을 $H(X) = \frac{1}{1+e^{W^{T}X}}​$ 로 변형할 수 있다. 

---

### Logistic Regression의 cost 함수 설명

$H(x)  = Wx + b$  : 기존 Linear Regression 함수는 변곡선이 완만하여 2차함수 모양의 형태를 가지므로 경사하강법을 적용할 수 있다. 하지만 Logistic Regression의 $H(X) = \frac{1}{1+e^{W^{T}X}}$ 기존 미분하기전에 그래프가 곡선이므로 2차 함수의 형태로 생기지 않아 경사하강법을 적용할 수 없다. 

따라서 global minimum값을 구하지 못하여 cost function도 바꿔야 한다. 

$cost(W) = \frac{1}{m}\sum{c(H(x), y)}$

$c(H(x), y) = \begin{cases} -log(H(x)), &\text{: y = 1} \\ -log(1-H(x)) &\text{ : y = 0}\end{cases}$

cost : 우리가 예측한 값의 차이 

ex) y = 1 인데 H(x) = 1 이면 cost는 0이다. 

​      y = 1 인데 H(x) = 0 이면 cost는 무한대이다. 

ex) y = 0 이면 H(x) = 0 이면 cost는 0이다. 

​      y = 0 이면 H(x) = 1 이면 cost는 무한대이다. 

따라서 이 두개의 그래프를 연결하면 Linear Regressoin 이차함수 같은 모양의 그래프가 된다. 

$C:(H(x), y) = -ylog(H(x)) - (1-y)log(1-H(x))$

$y = 1, c = -log(H(x))$

$y = 0, c =  -log(1-H(x))$

코스트가 주어졌으면 코스트가 minimize 하면 된다. 

$cost(W) = -\frac{1}{m}\sum ylog(H(x)) + (1-y)log(1-H(x))$

$W := W -\alpha\frac{\partial W}{\partial}cost(W)$

```python
# cost function 
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis)))
# Minimize
a = tf.Variable(0.1) # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```







