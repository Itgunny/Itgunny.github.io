## 손실 줄이기

### 1. 1 반복 방식

이전 정리에서 손실의 개념을 소개 했습니다. 이 모듈에서는 머신 러닝 모델이 반복을 통해 어떻게 손실을 줄이는지 알아보겠습니다. 

처음에는 임의의 지점에서 시작해서($'w_1$의 값은 0') 시스템이 손실 값을 알려줄 때까지 기다립니다. 그런 다음 다른 값을 추정해서 ($'w_1$의 값은 0.5') 손실 값을 확인합니다. 이렇게 계속 하다보면 손실률을 줄일 수 있습니다. 

다음 그림은 손실률을 줄이는 과정을 보여줍니다.

 ![특성과 라벨에서 모델과 예측으로 이동하는 주기입니다.](https://developers.google.com/machine-learning/crash-course/images/GradientDescentDiagram.svg?hl=ko)

그림 1. 반복 방식의 모델.

반복 전략은 주로 대규모 데이터 세트에 적용하기 용이하여 머신러닝에서 주로 사용되고 있다. 이 '모델'은 하나 이상의 특성을 입력하여 하나의 예측('y')을 출력한다. 하나의 특성을 가지고 하나의 예측을 반환하는 모델을 생각해보면

$y' = b + w_1x_1$

$b$와 $w_1$의 초기값은 무엇으로 설정해야 될까? 선형 회귀 문제에서는 초기값이 별로 중요하지 않다. 임의의 값을 정해도 되지만 일단은 다음값을 사용 한다. 

- $b = 0$
- $w_1 = 0$

최초 특성 값이 10이라고 가정 하면, 이 특성 값을 예측함수에 입력하면 다음과 같이 출력됩니다. 

```
y' = 0 + 0(10)
y' = 0
```

위의 다이어그램에서 '손실 계산' 과정은 이 모델에서 사용할 손실 함수입니다. 제곱손실함수를 사용한다고 가정하면, 손실함수는 두개의 입력 값을 취한다. 

- $y'$ : 특성 $x$에 대한 모델의 예측 값입니다. 
- $y$ : 특성 $x$에 대한 올바른 라벨입니다. 

결과로 다이어그램의 '매개변수 업데이트 계산' 과정에 도달하면 머신러닝 시스템은 손실 함수의 값을 검토하여 b와 w1의 새로운 값을 생성합니다. 일단은 매개변수를 변경하면서 생성해가며 모든 특성을 모든 라벨과 대조하여 재평가하여 손실함수의 손실 값이 가장 낮은 모델 매개변수를 발견할 때까지 반복 학습합니다. 보통 전체 손실이 변하지 않거나 변하는 폭이 좁을 때 까지 반복합니다. 이때 모델이 수렴하였다고 합니다. 

---

### 1. 2 경사 하강법

반복 방식 다이어그램에는 '매개변수 업데이트 계산'이 포함되어 있었고, 이 모호환 알고리즘을 좀더 실질적인 알고리즘으로 설명합니다. $w_1$의 가능한 모든 값에 대해 손실을 계산할 시간과 컴퓨팅 자료가 있다고 가정합니다. 손실과 $w1​$을 대응한 도표는 항상 볼록 함수 모양을 할 것입니다. 즉, 도표가 다음과 같이 볼록 함수 모양으로 나타납니다. 

![U자형 곡선 위의 두 번째 지점입니다. 이 지점은 최소값에 조금 더 가깝습니다.](https://developers.google.com/machine-learning/crash-course/images/convex.svg?hl=ko)

그림 2. 회귀 문제에서는 볼록 함수 모양의 손실 대 가중치 도표가 산출됩니다. 

볼록 문제에는 기울기가 정확히 0인 지점인 최소값이 하나만 존재합니다. 이 최소 값에서 손실 함수가 수렵합니다. 전체 데이터 세트에 대해 상상할 수 있는 모든 $w1$ 값의 손실 함수를 계산하는 것은 수렴 지점을 찾는 데 비효율적인 방법입니다. 따라서 경사하강법을 사용해야 합니다. 

경사하강법의 첫 번째 단계는 $w_1$ 에 대한 시작 값(시작점)을 선택하는 것입니다. 시작점은 중요하지 않아 많은 알고리즘에서 $w1$ 을 0으로 설정하거나 임의의 값을 선택합니다. 

![U자형 곡선 위의 두 번째 지점입니다. 이 지점은 최소값에 조금 더 가깝습니다.](https://developers.google.com/machine-learning/crash-course/images/GradientDescentStartingPoint.svg?hl=ko)  

그림 3. 경사하강법의 시작점 

시작점에서 곡선의 기울기를 계산한후 기울기는는 편미분의 벡터로서, 어느방향이 더 정확한지 혹은 더 부정확한지 알려준다. 그림 3에 나온 것과 같이 단일 가중치에 대한 손실의 기울기는 미분 값이다. 



기울기는 벡터이고, 다음 두가지 특성을 모두 가지고 있다. 

- 방향
- 크기

기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향을 향한다. 경사하강법 알고리즘은 가능한 빨리 손실을 줄이기 위해 기울기의 반대 방향으로 이동한다. 

![U자형 곡선 위의 두 번째 지점입니다. 이 지점은 최소값에 조금 더 가깝습니다.](https://developers.google.com/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg?hl=ko)

그림 4. 경사하강법은 음의 기울기를 사용한다.

손실 함수 곡선의 다음 지점을 결정하기 위해 기울기가 음수면 +$w_1$ 값을 더하고 양수면 $-w_1$ 한다. 

![U자형 곡선 위의 두 번째 지점입니다. 이 지점은 최소값에 조금 더 가깝습니다.](https://developers.google.com/machine-learning/crash-course/images/GradientDescentGradientStep.svg?hl=ko)

그림 5. 기울기 보폭을 통해 손실 곡선의 다음 지점으로 이동합니다. 

그런 다음 경사하강법은 이 과정을 반복해 최소값에 점점 접근한다. 

---

### 1. 3 학습률

경사하강법 알고리즘은 기울기에 학습률또는 보폭이라 불리는 스칼라를 곱하여 다음 지점을 결정한다. 예를 들어 기울기가 2.5이고, 학습률이 0.01이면 경사하강법 알고리즘은 이전 지점으로 부터 0.025 떨어진 지점을 다음 지점으로 결정한다. 

초 매개변수는 프로그래머가 머신러닝 알고리즘에서 조정하는 값입니다. 대부분의 머신 러닝 프로그래머는 학습률을 미세 조정하는 데 상당한 시간을 소비합니다. 학습률을 너무 작게 설정하면 학습시간이 오래 걸린다. 

![동일한 U자형 곡선입니다. 여러 개의 지점이 서로 가까이 붙어 있고 자취가 U자형 곡선의 바닥 쪽으로 매우 느리게 진행하고 있습니다.](https://developers.google.com/machine-learning/crash-course/images/LearningRateTooSmall.svg?hl=ko)

그림 6. 학습률이 너무 작다. 

반대로 학습률을 너무 크게 설정하면 양자역학 실험을 잘못한 것처럼 다음 지점이 곡선의 최저점을 무질서하게 이탈할 우려가 있습니다. 

![동일한 U자형 곡선입니다. 이 곡선에는 지점이 매우 적습니다. 점의 자취가 U자형 곡선의 바닥을 가로질러 이동하고 다시 위로 향하고 있습니다.](https://developers.google.com/machine-learning/crash-course/images/LearningRateTooLarge.svg?hl=ko)

그림 7. 학습률이 너무 큽니다. 

모든 회귀 문제에는 골디락스 학습률이 있습니다. 골디락스 값은 손실 함수가 얼마나 평탄한지 여부와 관련 있습니다. 손실 함수의 기울기가 작다면 더 큰 학습률을 시도해 볼 수 있습니다. 이렇게 작은 기울기를 보완하고 더 큰 보폭을 만들어 낼 수 있습니다. 

![동일한 U자형 곡선입니다. 점의 자취가 약 8개 보폭을 거쳐 최저점에 도달합니다.](https://developers.google.com/machine-learning/crash-course/images/LearningRateJustRight.svg?hl=ko)

---

### 손실 줄이기 : 확률적 경사하강법

경사하강법에서 배치는 단일 반복에서 기울기를 계싼하는 데 사용하는 예의 총 개수입니다. 지금 까지 배치가 전체 데이터 세트라고 가정했습니다. 하지만 Google 규모의 작업에서는 데이터 세트에 수십억, 수천억 개의 예가 포함되는 경우가 많습니다. 따라서 배치가 거대해질 수 있습니다. 배치가 너무 커지면 단일 반복으로도 계산하는 데 오랜 시간이 걸립니다. 

무작위로 샘플링된 예가 포함된 대량의 데이터 세트에는 중복 데이터가 포함되어 있을 수 있습니다. 실제로 배치크기가 커지면 중복의 가능성도 그만큼 높아집니다. 적당한 중복성은 노이즈가 있는 기울기를 평활화하는 데 유용할 수 있지만, 배치가 거대해지면 예측성이 훨씬 높은 값이 대용량 배치에 비해 덜포함되는 경향이 있습니다. 

그러면 훨씬 적은 계산으로 적절한 기울기를 얻을 수 있는 방법은 무엇인가? 데이터 세트에서 예를 무작위로 선택하여, 적은 데이터 세트로 중요한 평균값을 추정할 수 있다. 확률적 경사하강법(SGD)은 이 아이디어를 더욱 확장한 것으로, 반복당 하나의 예(배치 크기 1)만을 사용합니다. 반복이 충분하면 SGD가 효과는 있지만 오차가 너무 심하다. 확률적이라는 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택 된다는 것이다. 

미니 배치 확률적 경사하강법(미니 배치 SGD)는 전체 배치 반복과 SGD 간의 절충안이다. 미니 배치는 일반적으로 무작위로 선택한 10개에서 1,000개 사이의 예로 구성되고, 오차를 줄이면서도 전체 배치보다는 더 효율적이다. 

